{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tushar8049/Sentiment-Classification/blob/master/Sentiment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwlX7ndUCUo1",
        "colab_type": "code",
        "outputId": "943a358f-5425-4624-9f8e-3e427a3d85c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip install sklearn\n",
        "!pip install torchvision\n",
        "!pip install pytorch-nlp\n",
        "!git clone https://github.com/tushar8049/Sentiment-Classification.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.4.1+cu100)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.3)\n",
            "Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.3.0+cu100)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting pytorch-nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/ae/b6d18c3f37da5a78e83701469e6153811f4b0ecb3f9387bb3e9a65ca48ee/pytorch_nlp-0.4.1-py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.17.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (2.21.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->pytorch-nlp) (2.6.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-nlp) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->pytorch-nlp) (1.12.0)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.4.1\n",
            "Cloning into 'Sentiment-Classification'...\n",
            "remote: Enumerating objects: 2419, done.\u001b[K\n",
            "remote: Counting objects: 100% (2419/2419), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2380/2380), done.\u001b[K\n",
            "remote: Total 2419 (delta 31), reused 2416 (delta 28), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2419/2419), 5.70 MiB | 13.30 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI16BP2VCGEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "import sys\n",
        "import os\n",
        "import io\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "from torchnlp import encoders\n",
        "from torchnlp.encoders import text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGYkq0leJh-T",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Loading the Data in an Array Temporarily after iterating through the files. Along with that encoding of the String is done giving a unique integer to the words in the Corpus. \n",
        "\n",
        "*   **raw_review:** determines the bag of sentences both negative and positive classification\n",
        "\n",
        "*   **encoded_review:** determines the encoded bag of sentences both negative and positive classification\n",
        "\n",
        "*   **raw_label:** determines the label {negative, positive}\n",
        "\n",
        "*   **encoded_label:** determines the label {0, 1}\n",
        "\n",
        "**Location of Data:** Data is primararily hosted on the Github and then the Github repo is cloned and the data is transfered to the the Local Colab Environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNnywg5vEsD5",
        "colab_type": "code",
        "outputId": "2f877c0f-e94e-48d8-a4bd-a2fe0ca6843c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "raw_review = []\n",
        "raw_label = []\n",
        "\n",
        "encoded_review = []\n",
        "encoded_label = []\n",
        "\n",
        "vocab = {}\n",
        "vocab_counter = 1\n",
        "maximum_file_length = 0\n",
        "# Negative Dataset\n",
        "path = \"/content/Sentiment-Classification/movie_reviews/neg\"\n",
        "for filename in os.listdir(path):\n",
        "    filename = os.path.join(path, filename)\n",
        "    reader = io.open(filename)\n",
        "    line = reader.readline()\n",
        "    bagOfWords = []\n",
        "    bagOfEncode = []\n",
        "    while line:\n",
        "        sentence = re.sub('[^A-Za-z0-9]+', ' ', line)\n",
        "        sentence = re.sub(' +', ' ',sentence)\n",
        "        words = sentence.strip().lstrip().rstrip().split(\" \")\n",
        "        for word in words:\n",
        "            if word not in vocab.keys():\n",
        "                vocab[word] = vocab_counter\n",
        "                vocab_counter += 1\n",
        "            bagOfEncode.append(vocab[word])\n",
        "            bagOfWords.append(word)\n",
        "        line = reader.readline()\n",
        "    maximum_file_length = max(maximum_file_length, len(bagOfEncode))\n",
        "    raw_review.append(bagOfWords)\n",
        "    encoded_review.append(bagOfEncode)\n",
        "    raw_label.append(\"NEGATIVE\")\n",
        "    encoded_label.append(0)\n",
        "\n",
        "# Positive Dataset\n",
        "path = \"/content/Sentiment-Classification/movie_reviews/pos\"\n",
        "for filename in os.listdir(path):\n",
        "    filename = os.path.join(path, filename)\n",
        "    reader = io.open(filename)\n",
        "    line = reader.readline()\n",
        "    bagOfWords = []\n",
        "    bagOfEncode = []\n",
        "    while line:\n",
        "        sentence = re.sub('[^A-Za-z0-9]+', ' ', line)\n",
        "        sentence = re.sub(' +', ' ',sentence)\n",
        "        words = sentence.strip().lstrip().rstrip().split(\" \")\n",
        "        for word in words:\n",
        "            if word not in vocab.keys():\n",
        "                vocab[word] = vocab_counter\n",
        "                vocab_counter += 1\n",
        "            bagOfEncode.append(vocab[word])\n",
        "            bagOfWords.append(word)\n",
        "        line = reader.readline()\n",
        "    maximum_file_length = max(maximum_file_length, len(bagOfEncode))\n",
        "    raw_review.append(bagOfWords)\n",
        "    encoded_review.append(bagOfEncode)\n",
        "    raw_label.append(\"POSITIVE\")\n",
        "    encoded_label.append(1)\n",
        "\n",
        "print(\"Length of Data: \", len(raw_review))\n",
        "print(\"Length of Labels: \", len(raw_label))\n",
        "\n",
        "print(\"Length of Data: \", len(encoded_review))\n",
        "print(\"Length of Labels: \", len(encoded_label))\n",
        "\n",
        "print(\"Vocab Length: \",len(vocab))\n",
        "print(\"File with maximum length: \", maximum_file_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Data:  2000\n",
            "Length of Labels:  2000\n",
            "Length of Data:  2000\n",
            "Length of Labels:  2000\n",
            "Vocab Length:  39400\n",
            "File with maximum length:  2462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crpjsiIxSJ6d",
        "colab_type": "text"
      },
      "source": [
        "Data Loading is done for the Test Data and seperate Arrays have been created for that.\n",
        "*   **test_raw_review:** determines the bag of sentences both negative and positive classification\n",
        "\n",
        "*   **test_encoded_review:** determines the encoded bag of sentences both negative and positive classification\n",
        "\n",
        "*   **test_raw_label:** determines the label {negative, positive}\n",
        "\n",
        "*   **test_encoded_label:** determines the label {0, 1}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voD71J1Malt0",
        "colab_type": "code",
        "outputId": "baf3475b-f9ba-4325-9723-dcd6e0e87bce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "test_raw_review = []\n",
        "test_raw_label = []\n",
        "\n",
        "test_encoded_review = []\n",
        "test_encoded_label = []\n",
        "\n",
        "\n",
        "# Negative Dataset\n",
        "path = \"/content/Sentiment-Classification/movie_reviews/test/neg\"\n",
        "for filename in os.listdir(path):\n",
        "    filename = os.path.join(path, filename)\n",
        "    reader = io.open(filename)\n",
        "    line = reader.readline()\n",
        "    bagOfWords = []\n",
        "    bagOfEncode = []\n",
        "    while line:\n",
        "        sentence = re.sub('[^A-Za-z0-9]+', ' ', line)\n",
        "        sentence = re.sub(' +', ' ',sentence)\n",
        "        words = sentence.strip().lstrip().rstrip().split(\" \")\n",
        "        for word in words:\n",
        "            if word not in vocab.keys():\n",
        "                bagOfEncode.append(vocab[word])\n",
        "            else:\n",
        "                bagOfEncode.append(0)\n",
        "            bagOfWords.append(word)\n",
        "        line = reader.readline()\n",
        "    test_raw_review.append(bagOfWords)\n",
        "    test_encoded_review.append(bagOfEncode)\n",
        "    test_raw_label.append(\"NEGATIVE\")\n",
        "    test_encoded_label.append(0)\n",
        "\n",
        "# Positive Dataset\n",
        "path = \"/content/Sentiment-Classification/movie_reviews/test/pos\"\n",
        "for filename in os.listdir(path):\n",
        "    filename = os.path.join(path, filename)\n",
        "    reader = io.open(filename)\n",
        "    line = reader.readline()\n",
        "    bagOfWords = []\n",
        "    bagOfEncode = []\n",
        "    while line:\n",
        "        sentence = re.sub('[^A-Za-z0-9]+', ' ', line)\n",
        "        sentence = re.sub(' +', ' ',sentence)\n",
        "        words = sentence.strip().lstrip().rstrip().split(\" \")\n",
        "        for word in words:\n",
        "            if word not in vocab.keys():\n",
        "                bagOfEncode.append(vocab[word])\n",
        "            else:\n",
        "                bagOfEncode.append(0)\n",
        "            bagOfWords.append(word)\n",
        "        line = reader.readline()\n",
        "    test_raw_review.append(bagOfWords)\n",
        "    test_encoded_review.append(bagOfEncode)\n",
        "    test_raw_label.append(\"POSITIVE\")\n",
        "    test_encoded_label.append(1)\n",
        "\n",
        "print(\"Length of Test Data: \", len(test_raw_review))\n",
        "print(\"Length of Test Labels: \", len(test_raw_label))\n",
        "\n",
        "print(\"Length of Test Encoded Data: \", len(test_encoded_review))\n",
        "print(\"Length of Test Encoded Labels: \", len(test_encoded_label))\n",
        "\n",
        "print(\"File with maximum length: \", maximum_file_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Test Data:  100\n",
            "Length of Test Labels:  100\n",
            "Length of Test Encoded Data:  100\n",
            "Length of Test Encoded Labels:  100\n",
            "File with maximum length:  2462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmJfl3-o9flh",
        "colab_type": "text"
      },
      "source": [
        "Padding **encoded_review** so that the dimension of the Array of Arrays become equal as not all files have equal number of words and Neural Net requires input of same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UMMfE0cWNZk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_review = np.asarray([np.pad(a, (0, maximum_file_length - len(a)), 'constant', constant_values=0) for a in encoded_review])\n",
        "encoded_review\n",
        "\n",
        "encoded_label = np.asarray([encoded_label])\n",
        "\n",
        "\n",
        "test_encoded_review = np.asarray([np.pad(a, (0, maximum_file_length - len(a)), 'constant', constant_values=0) for a in test_encoded_review])\n",
        "test_encoded_review\n",
        "\n",
        "test_encoded_label = np.asarray([test_encoded_label])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6kvMu4DAM6l",
        "colab_type": "code",
        "outputId": "1ad218a5-0d0e-45d9-d4dc-73abec29cf29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "encoded_review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,    2,    3, ...,    0,    0,    0],\n",
              "       [ 292,  170,  197, ...,    0,    0,    0],\n",
              "       [ 156,  447,   11, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [6957,   49,   23, ...,    0,    0,    0],\n",
              "       [   1,    2,  165, ...,    0,    0,    0],\n",
              "       [ 140,  257,  471, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV4ilIsLSvMt",
        "colab_type": "text"
      },
      "source": [
        "Spliting the Dataset into Training and Validating Data ans also Shuffling both the test data and training and validating data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF_vdr_LKoSH",
        "colab_type": "code",
        "outputId": "a21f7b93-9a09-4fdf-f546-7a145214e576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"Review: \", encoded_review.shape)\n",
        "\n",
        "combined_encoded_data = np.concatenate((encoded_review, encoded_label.T), axis=1)\n",
        "combined_encoded_data\n",
        "\n",
        "print(\"Combined Data for Shuffling: \", combined_encoded_data.shape)\n",
        "\n",
        "np.random.shuffle(combined_encoded_data)\n",
        "\n",
        "\n",
        "train_ratio = 0.8\n",
        "valid_ratio = 0.2\n",
        "\n",
        "train_cutoff = int(len(combined_encoded_data) * train_ratio)\n",
        "\n",
        "training_data = combined_encoded_data[:train_cutoff]\n",
        "validating_data = combined_encoded_data[train_cutoff:]\n",
        "\n",
        "training_review = training_data[:,0:-1]\n",
        "training_label = training_data[:,-1]\n",
        "\n",
        "validating_review = validating_data[:,0:-1]\n",
        "validating_label = validating_data[:,-1]\n",
        "\n",
        "print(\"Training Data Shape: \", training_review.shape)\n",
        "print(\"Training Label Shape: \", training_label.shape)\n",
        "\n",
        "print(\"Validating Data Shape: \", validating_review.shape)\n",
        "print(\"Validating Label Shape: \", validating_label.shape)\n",
        "\n",
        "\n",
        "combined_test_encoded_data = np.concatenate((test_encoded_review, test_encoded_label.T), axis=1)\n",
        "combined_test_encoded_data\n",
        "\n",
        "np.random.shuffle(combined_test_encoded_data)\n",
        "test_review = combined_test_encoded_data[:,0:-1]\n",
        "test_label = combined_test_encoded_data[:,-1]\n",
        "print(\"Testing Data Shape: \", test_review.shape)\n",
        "print(\"Testing Label Shape: \", test_label.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review:  (2000, 2462)\n",
            "Combined Data for Shuffling:  (2000, 2463)\n",
            "Training Data Shape:  (1600, 2462)\n",
            "Training Label Shape:  (1600,)\n",
            "Validating Data Shape:  (400, 2462)\n",
            "Validating Label Shape:  (400,)\n",
            "Testing Data Shape:  (100, 2462)\n",
            "Testing Label Shape:  (100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh2xDyHl9hZL",
        "colab_type": "text"
      },
      "source": [
        "Creating an Embedding Dictionary and also transforming the encoded_review into a tensorEncoded format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig7JX9tyGR1H",
        "colab_type": "code",
        "outputId": "669ee2a4-e04d-45ea-a2e9-87fa0830bdbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "embedding = nn.Embedding(len(vocab)+1, 300)\n",
        "input = torch.LongTensor(encoded_review)\n",
        "\n",
        "embedded_vocab = {}\n",
        "embedded_vocab_array =[]\n",
        "for key in vocab.keys():\n",
        "    embedded_vocab[key] = embedding(torch.LongTensor([vocab[key]]))\n",
        "    embedded_vocab_array.append(embedded_vocab[key])\n",
        "    \n",
        "    \n",
        "tensorEncoded = embedding(input)\n",
        "tensorEncoded\n",
        "\n",
        "embedding_label = nn.Embedding(2, 300)\n",
        "input_label = torch.LongTensor(encoded_label)\n",
        "tensorEncoded_label = embedding_label(input_label)\n",
        "tensorEncoded_label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2124, -0.8379,  0.6749,  ...,  0.6871, -0.5017, -1.6020],\n",
              "         [-0.2124, -0.8379,  0.6749,  ...,  0.6871, -0.5017, -1.6020],\n",
              "         [-0.2124, -0.8379,  0.6749,  ...,  0.6871, -0.5017, -1.6020],\n",
              "         ...,\n",
              "         [ 0.0449, -2.2014,  1.9208,  ...,  0.6559,  0.4940,  0.9216],\n",
              "         [ 0.0449, -2.2014,  1.9208,  ...,  0.6559,  0.4940,  0.9216],\n",
              "         [ 0.0449, -2.2014,  1.9208,  ...,  0.6559,  0.4940,  0.9216]]],\n",
              "       grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTlzjAfsTJF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(torch.from_numpy(training_review), torch.from_numpy(training_label))\n",
        "valid_data = TensorDataset(torch.from_numpy(validating_review), torch.from_numpy(validating_label))\n",
        "test_data = TensorDataset(torch.from_numpy(test_review), torch.from_numpy(test_label))\n",
        "\n",
        "batch_size = 100\n",
        "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJq82NtfTSxL",
        "colab_type": "code",
        "outputId": "8dd15fae-3263-4fcc-942e-004f71b7d895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJt50ABIXeOK",
        "colab_type": "text"
      },
      "source": [
        "# **Baseline Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmRsqtyvXbNo",
        "colab_type": "code",
        "outputId": "fed1f76e-5a63-4dc3-c298-e9fbb7349ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Defineing Model\n",
        "model = nn.Sequential(nn.Linear(2462, 512),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(512, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256, 512),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(512, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 2),\n",
        "                      nn.Softmax())\n",
        "print(\"\\nModel Structure:\")\n",
        "print(model)\n",
        "\n",
        "\n",
        "# Defining Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "\n",
        "\n",
        "# Training the Model with the number of Epochs\n",
        "print(\"\\nModel Training:\")\n",
        "counter = 0\n",
        "epochs = 50\n",
        "print_every = 4\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for data, labels in train_loader:\n",
        "        counter += 1\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        labels = labels.float()\n",
        "        output = model.forward(data)\n",
        "        loss = criterion(output.float(), labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if counter % print_every == 0:\n",
        "            \n",
        "            num_correct = 0\n",
        "            \n",
        "            val_losses = []\n",
        "            for inputs, labels in valid_loader:\n",
        "                \n",
        "                inputs, labels = inputs.float(), labels.float()\n",
        "\n",
        "                output = model.forward(data)\n",
        "                \n",
        "                pred = torch.round(output.squeeze()) \n",
        "                \n",
        "                actual =[]\n",
        "                for x,y in pred:\n",
        "                    if x == 1:\n",
        "                        actual.append(0)\n",
        "                    else:\n",
        "                        actual.append(1)\n",
        "                actual = torch.FloatTensor(actual)\n",
        "                # compare predictions to true label\n",
        "                correct_tensor = actual.eq(labels.float())\n",
        "                correct = np.squeeze(correct_tensor.numpy())\n",
        "                num_correct += np.sum(correct)\n",
        "                val_loss = criterion(output.float(), labels.long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            \n",
        "            val_acc = num_correct/len(valid_loader.dataset)\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(np.mean(val_acc)))\n",
        "            \n",
        "            \n",
        "print(\"\\nTest Data Result:\")\n",
        "for inputs, labels in test_loader:\n",
        "                \n",
        "    inputs, labels = inputs.float(), labels.float()\n",
        "\n",
        "    output = model.forward(data)\n",
        "    \n",
        "    pred = torch.round(output.squeeze()) \n",
        "    \n",
        "    actual =[]\n",
        "    for x,y in pred:\n",
        "        if x == 1:\n",
        "            actual.append(0)\n",
        "        else:\n",
        "            actual.append(1)\n",
        "    actual = torch.FloatTensor(actual)\n",
        "    \n",
        "    correct_tensor = actual.eq(labels.float())\n",
        "    correct = np.squeeze(correct_tensor.numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    val_loss = criterion(output.float(), labels.long())\n",
        "\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "\n",
        "val_acc = num_correct/len(valid_loader.dataset)\n",
        "print(\"Test Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "      \"Test Acc: {:.6f}\".format(np.mean(val_acc)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model Structure:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2462, out_features=512, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (9): ReLU()\n",
            "  (10): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (11): ReLU()\n",
            "  (12): Linear(in_features=256, out_features=512, bias=True)\n",
            "  (13): ReLU()\n",
            "  (14): Linear(in_features=512, out_features=64, bias=True)\n",
            "  (15): ReLU()\n",
            "  (16): Linear(in_features=64, out_features=2, bias=True)\n",
            "  (17): Softmax(dim=None)\n",
            ")\n",
            "\n",
            "Model Training:\n",
            "Epoch: 1/50... Step: 4... Loss: 0.728305... Val Loss: 0.715395 Val Acc: 0.520000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/50... Step: 8... Loss: 0.699181... Val Loss: 0.709362 Val Acc: 0.510000\n",
            "Epoch: 1/50... Step: 12... Loss: 0.716945... Val Loss: 0.705342 Val Acc: 0.510000\n",
            "Epoch: 1/50... Step: 16... Loss: 0.671973... Val Loss: 0.721591 Val Acc: 0.510000\n",
            "Epoch: 2/50... Step: 20... Loss: 0.729916... Val Loss: 0.730440 Val Acc: 0.495000\n",
            "Epoch: 2/50... Step: 24... Loss: 0.634154... Val Loss: 0.720465 Val Acc: 0.515000\n",
            "Epoch: 2/50... Step: 28... Loss: 0.665011... Val Loss: 0.719696 Val Acc: 0.480000\n",
            "Epoch: 2/50... Step: 32... Loss: 0.663413... Val Loss: 0.714078 Val Acc: 0.505000\n",
            "Epoch: 3/50... Step: 36... Loss: 0.633806... Val Loss: 0.708405 Val Acc: 0.530000\n",
            "Epoch: 3/50... Step: 40... Loss: 0.650462... Val Loss: 0.692378 Val Acc: 0.550000\n",
            "Epoch: 3/50... Step: 44... Loss: 0.629027... Val Loss: 0.727637 Val Acc: 0.480000\n",
            "Epoch: 3/50... Step: 48... Loss: 0.617890... Val Loss: 0.736043 Val Acc: 0.500000\n",
            "Epoch: 4/50... Step: 52... Loss: 0.611092... Val Loss: 0.717330 Val Acc: 0.495000\n",
            "Epoch: 4/50... Step: 56... Loss: 0.574095... Val Loss: 0.742260 Val Acc: 0.490000\n",
            "Epoch: 4/50... Step: 60... Loss: 0.605673... Val Loss: 0.733265 Val Acc: 0.515000\n",
            "Epoch: 4/50... Step: 64... Loss: 0.586869... Val Loss: 0.719790 Val Acc: 0.525000\n",
            "Epoch: 5/50... Step: 68... Loss: 0.562608... Val Loss: 0.719795 Val Acc: 0.540000\n",
            "Epoch: 5/50... Step: 72... Loss: 0.581980... Val Loss: 0.720832 Val Acc: 0.505000\n",
            "Epoch: 5/50... Step: 76... Loss: 0.571398... Val Loss: 0.757168 Val Acc: 0.510000\n",
            "Epoch: 5/50... Step: 80... Loss: 0.561410... Val Loss: 0.734685 Val Acc: 0.495000\n",
            "Epoch: 6/50... Step: 84... Loss: 0.561743... Val Loss: 0.738028 Val Acc: 0.505000\n",
            "Epoch: 6/50... Step: 88... Loss: 0.551940... Val Loss: 0.727232 Val Acc: 0.515000\n",
            "Epoch: 6/50... Step: 92... Loss: 0.631617... Val Loss: 0.759276 Val Acc: 0.495000\n",
            "Epoch: 6/50... Step: 96... Loss: 0.569236... Val Loss: 0.759478 Val Acc: 0.465000\n",
            "Epoch: 7/50... Step: 100... Loss: 0.550517... Val Loss: 0.729701 Val Acc: 0.505000\n",
            "Epoch: 7/50... Step: 104... Loss: 0.544390... Val Loss: 0.736038 Val Acc: 0.520000\n",
            "Epoch: 7/50... Step: 108... Loss: 0.543194... Val Loss: 0.723617 Val Acc: 0.505000\n",
            "Epoch: 7/50... Step: 112... Loss: 0.564181... Val Loss: 0.736676 Val Acc: 0.485000\n",
            "Epoch: 8/50... Step: 116... Loss: 0.512455... Val Loss: 0.744669 Val Acc: 0.500000\n",
            "Epoch: 8/50... Step: 120... Loss: 0.508898... Val Loss: 0.732978 Val Acc: 0.540000\n",
            "Epoch: 8/50... Step: 124... Loss: 0.515543... Val Loss: 0.767464 Val Acc: 0.500000\n",
            "Epoch: 8/50... Step: 128... Loss: 0.520811... Val Loss: 0.763041 Val Acc: 0.490000\n",
            "Epoch: 9/50... Step: 132... Loss: 0.435655... Val Loss: 0.782196 Val Acc: 0.485000\n",
            "Epoch: 9/50... Step: 136... Loss: 0.447990... Val Loss: 0.759543 Val Acc: 0.505000\n",
            "Epoch: 9/50... Step: 140... Loss: 0.529432... Val Loss: 0.747210 Val Acc: 0.525000\n",
            "Epoch: 9/50... Step: 144... Loss: 0.459515... Val Loss: 0.767336 Val Acc: 0.485000\n",
            "Epoch: 10/50... Step: 148... Loss: 0.428442... Val Loss: 0.772951 Val Acc: 0.485000\n",
            "Epoch: 10/50... Step: 152... Loss: 0.441624... Val Loss: 0.801136 Val Acc: 0.460000\n",
            "Epoch: 10/50... Step: 156... Loss: 0.453667... Val Loss: 0.746204 Val Acc: 0.525000\n",
            "Epoch: 10/50... Step: 160... Loss: 0.476583... Val Loss: 0.757991 Val Acc: 0.510000\n",
            "Epoch: 11/50... Step: 164... Loss: 0.401220... Val Loss: 0.746383 Val Acc: 0.540000\n",
            "Epoch: 11/50... Step: 168... Loss: 0.454511... Val Loss: 0.772688 Val Acc: 0.490000\n",
            "Epoch: 11/50... Step: 172... Loss: 0.604078... Val Loss: 0.779618 Val Acc: 0.475000\n",
            "Epoch: 11/50... Step: 176... Loss: 0.469981... Val Loss: 0.764466 Val Acc: 0.505000\n",
            "Epoch: 12/50... Step: 180... Loss: 0.421747... Val Loss: 0.775149 Val Acc: 0.500000\n",
            "Epoch: 12/50... Step: 184... Loss: 0.375539... Val Loss: 0.787775 Val Acc: 0.495000\n",
            "Epoch: 12/50... Step: 188... Loss: 0.396083... Val Loss: 0.778086 Val Acc: 0.510000\n",
            "Epoch: 12/50... Step: 192... Loss: 0.391898... Val Loss: 0.790637 Val Acc: 0.490000\n",
            "Epoch: 13/50... Step: 196... Loss: 0.374732... Val Loss: 0.737391 Val Acc: 0.560000\n",
            "Epoch: 13/50... Step: 200... Loss: 0.363786... Val Loss: 0.808113 Val Acc: 0.485000\n",
            "Epoch: 13/50... Step: 204... Loss: 0.365696... Val Loss: 0.807233 Val Acc: 0.470000\n",
            "Epoch: 13/50... Step: 208... Loss: 0.379555... Val Loss: 0.790303 Val Acc: 0.480000\n",
            "Epoch: 14/50... Step: 212... Loss: 0.367669... Val Loss: 0.833962 Val Acc: 0.455000\n",
            "Epoch: 14/50... Step: 216... Loss: 0.355166... Val Loss: 0.804793 Val Acc: 0.490000\n",
            "Epoch: 14/50... Step: 220... Loss: 0.383939... Val Loss: 0.810497 Val Acc: 0.480000\n",
            "Epoch: 14/50... Step: 224... Loss: 0.346352... Val Loss: 0.791984 Val Acc: 0.510000\n",
            "Epoch: 15/50... Step: 228... Loss: 0.339079... Val Loss: 0.792913 Val Acc: 0.505000\n",
            "Epoch: 15/50... Step: 232... Loss: 0.355149... Val Loss: 0.795700 Val Acc: 0.490000\n",
            "Epoch: 15/50... Step: 236... Loss: 0.334820... Val Loss: 0.803163 Val Acc: 0.495000\n",
            "Epoch: 15/50... Step: 240... Loss: 0.340173... Val Loss: 0.773236 Val Acc: 0.525000\n",
            "Epoch: 16/50... Step: 244... Loss: 0.333864... Val Loss: 0.744960 Val Acc: 0.560000\n",
            "Epoch: 16/50... Step: 248... Loss: 0.340273... Val Loss: 0.783481 Val Acc: 0.520000\n",
            "Epoch: 16/50... Step: 252... Loss: 0.359037... Val Loss: 0.782652 Val Acc: 0.520000\n",
            "Epoch: 16/50... Step: 256... Loss: 0.332275... Val Loss: 0.823966 Val Acc: 0.480000\n",
            "Epoch: 17/50... Step: 260... Loss: 0.330092... Val Loss: 0.827873 Val Acc: 0.475000\n",
            "Epoch: 17/50... Step: 264... Loss: 0.338517... Val Loss: 0.803492 Val Acc: 0.495000\n",
            "Epoch: 17/50... Step: 268... Loss: 0.337912... Val Loss: 0.793936 Val Acc: 0.510000\n",
            "Epoch: 17/50... Step: 272... Loss: 0.349808... Val Loss: 0.824202 Val Acc: 0.480000\n",
            "Epoch: 18/50... Step: 276... Loss: 0.333694... Val Loss: 0.766376 Val Acc: 0.540000\n",
            "Epoch: 18/50... Step: 280... Loss: 0.353985... Val Loss: 0.771285 Val Acc: 0.530000\n",
            "Epoch: 18/50... Step: 284... Loss: 0.329228... Val Loss: 0.792907 Val Acc: 0.515000\n",
            "Epoch: 18/50... Step: 288... Loss: 0.326245... Val Loss: 0.773174 Val Acc: 0.530000\n",
            "Epoch: 19/50... Step: 292... Loss: 0.327964... Val Loss: 0.803579 Val Acc: 0.505000\n",
            "Epoch: 19/50... Step: 296... Loss: 0.341541... Val Loss: 0.745700 Val Acc: 0.565000\n",
            "Epoch: 19/50... Step: 300... Loss: 0.326167... Val Loss: 0.858571 Val Acc: 0.445000\n",
            "Epoch: 19/50... Step: 304... Loss: 0.333147... Val Loss: 0.803252 Val Acc: 0.505000\n",
            "Epoch: 20/50... Step: 308... Loss: 0.330494... Val Loss: 0.817156 Val Acc: 0.490000\n",
            "Epoch: 20/50... Step: 312... Loss: 0.349183... Val Loss: 0.826034 Val Acc: 0.480000\n",
            "Epoch: 20/50... Step: 316... Loss: 0.337461... Val Loss: 0.816687 Val Acc: 0.490000\n",
            "Epoch: 20/50... Step: 320... Loss: 0.323408... Val Loss: 0.845023 Val Acc: 0.460000\n",
            "Epoch: 21/50... Step: 324... Loss: 0.332217... Val Loss: 0.817173 Val Acc: 0.490000\n",
            "Epoch: 21/50... Step: 328... Loss: 0.318965... Val Loss: 0.785805 Val Acc: 0.525000\n",
            "Epoch: 21/50... Step: 332... Loss: 0.328329... Val Loss: 0.774192 Val Acc: 0.535000\n",
            "Epoch: 21/50... Step: 336... Loss: 0.331304... Val Loss: 0.815290 Val Acc: 0.485000\n",
            "Epoch: 22/50... Step: 340... Loss: 0.320212... Val Loss: 0.813955 Val Acc: 0.495000\n",
            "Epoch: 22/50... Step: 344... Loss: 0.331581... Val Loss: 0.790140 Val Acc: 0.515000\n",
            "Epoch: 22/50... Step: 348... Loss: 0.341028... Val Loss: 0.784126 Val Acc: 0.525000\n",
            "Epoch: 22/50... Step: 352... Loss: 0.317340... Val Loss: 0.855509 Val Acc: 0.455000\n",
            "Epoch: 23/50... Step: 356... Loss: 0.319072... Val Loss: 0.783822 Val Acc: 0.525000\n",
            "Epoch: 23/50... Step: 360... Loss: 0.320033... Val Loss: 0.790301 Val Acc: 0.520000\n",
            "Epoch: 23/50... Step: 364... Loss: 0.318421... Val Loss: 0.804647 Val Acc: 0.505000\n",
            "Epoch: 23/50... Step: 368... Loss: 0.317973... Val Loss: 0.797247 Val Acc: 0.510000\n",
            "Epoch: 24/50... Step: 372... Loss: 0.318150... Val Loss: 0.823980 Val Acc: 0.485000\n",
            "Epoch: 24/50... Step: 376... Loss: 0.326034... Val Loss: 0.825642 Val Acc: 0.485000\n",
            "Epoch: 24/50... Step: 380... Loss: 0.319034... Val Loss: 0.879007 Val Acc: 0.430000\n",
            "Epoch: 24/50... Step: 384... Loss: 0.315681... Val Loss: 0.781660 Val Acc: 0.530000\n",
            "Epoch: 25/50... Step: 388... Loss: 0.318387... Val Loss: 0.839606 Val Acc: 0.470000\n",
            "Epoch: 25/50... Step: 392... Loss: 0.328753... Val Loss: 0.775390 Val Acc: 0.535000\n",
            "Epoch: 25/50... Step: 396... Loss: 0.317152... Val Loss: 0.809526 Val Acc: 0.500000\n",
            "Epoch: 25/50... Step: 400... Loss: 0.328659... Val Loss: 0.837221 Val Acc: 0.470000\n",
            "Epoch: 26/50... Step: 404... Loss: 0.315459... Val Loss: 0.811664 Val Acc: 0.500000\n",
            "Epoch: 26/50... Step: 408... Loss: 0.326249... Val Loss: 0.751648 Val Acc: 0.560000\n",
            "Epoch: 26/50... Step: 412... Loss: 0.327026... Val Loss: 0.816374 Val Acc: 0.495000\n",
            "Epoch: 26/50... Step: 416... Loss: 0.315396... Val Loss: 0.796172 Val Acc: 0.515000\n",
            "Epoch: 27/50... Step: 420... Loss: 0.317378... Val Loss: 0.827203 Val Acc: 0.485000\n",
            "Epoch: 27/50... Step: 424... Loss: 0.316004... Val Loss: 0.831438 Val Acc: 0.480000\n",
            "Epoch: 27/50... Step: 428... Loss: 0.316007... Val Loss: 0.850348 Val Acc: 0.460000\n",
            "Epoch: 27/50... Step: 432... Loss: 0.318698... Val Loss: 0.779811 Val Acc: 0.530000\n",
            "Epoch: 28/50... Step: 436... Loss: 0.336695... Val Loss: 0.854860 Val Acc: 0.455000\n",
            "Epoch: 28/50... Step: 440... Loss: 0.314919... Val Loss: 0.801721 Val Acc: 0.510000\n",
            "Epoch: 28/50... Step: 444... Loss: 0.316451... Val Loss: 0.801855 Val Acc: 0.510000\n",
            "Epoch: 28/50... Step: 448... Loss: 0.315849... Val Loss: 0.800567 Val Acc: 0.510000\n",
            "Epoch: 29/50... Step: 452... Loss: 0.315291... Val Loss: 0.811345 Val Acc: 0.500000\n",
            "Epoch: 29/50... Step: 456... Loss: 0.314502... Val Loss: 0.802305 Val Acc: 0.510000\n",
            "Epoch: 29/50... Step: 460... Loss: 0.327003... Val Loss: 0.805372 Val Acc: 0.505000\n",
            "Epoch: 29/50... Step: 464... Loss: 0.327922... Val Loss: 0.746749 Val Acc: 0.565000\n",
            "Epoch: 30/50... Step: 468... Loss: 0.315288... Val Loss: 0.806300 Val Acc: 0.505000\n",
            "Epoch: 30/50... Step: 472... Loss: 0.314867... Val Loss: 0.866376 Val Acc: 0.445000\n",
            "Epoch: 30/50... Step: 476... Loss: 0.325549... Val Loss: 0.812321 Val Acc: 0.500000\n",
            "Epoch: 30/50... Step: 480... Loss: 0.315412... Val Loss: 0.812464 Val Acc: 0.500000\n",
            "Epoch: 31/50... Step: 484... Loss: 0.315077... Val Loss: 0.786781 Val Acc: 0.525000\n",
            "Epoch: 31/50... Step: 488... Loss: 0.314833... Val Loss: 0.791779 Val Acc: 0.520000\n",
            "Epoch: 31/50... Step: 492... Loss: 0.325204... Val Loss: 0.812325 Val Acc: 0.500000\n",
            "Epoch: 31/50... Step: 496... Loss: 0.325194... Val Loss: 0.842219 Val Acc: 0.470000\n",
            "Epoch: 32/50... Step: 500... Loss: 0.315759... Val Loss: 0.825619 Val Acc: 0.485000\n",
            "Epoch: 32/50... Step: 504... Loss: 0.314701... Val Loss: 0.817028 Val Acc: 0.495000\n",
            "Epoch: 32/50... Step: 508... Loss: 0.325302... Val Loss: 0.800596 Val Acc: 0.510000\n",
            "Epoch: 32/50... Step: 512... Loss: 0.334982... Val Loss: 0.835863 Val Acc: 0.475000\n",
            "Epoch: 33/50... Step: 516... Loss: 0.324813... Val Loss: 0.866790 Val Acc: 0.445000\n",
            "Epoch: 33/50... Step: 520... Loss: 0.315045... Val Loss: 0.811735 Val Acc: 0.500000\n",
            "Epoch: 33/50... Step: 524... Loss: 0.314836... Val Loss: 0.817117 Val Acc: 0.495000\n",
            "Epoch: 33/50... Step: 528... Loss: 0.315188... Val Loss: 0.766933 Val Acc: 0.545000\n",
            "Epoch: 34/50... Step: 532... Loss: 0.324893... Val Loss: 0.782343 Val Acc: 0.530000\n",
            "Epoch: 34/50... Step: 536... Loss: 0.315553... Val Loss: 0.796275 Val Acc: 0.515000\n",
            "Epoch: 34/50... Step: 540... Loss: 0.314558... Val Loss: 0.846928 Val Acc: 0.465000\n",
            "Epoch: 34/50... Step: 544... Loss: 0.314163... Val Loss: 0.812546 Val Acc: 0.500000\n",
            "Epoch: 35/50... Step: 548... Loss: 0.314469... Val Loss: 0.787298 Val Acc: 0.525000\n",
            "Epoch: 35/50... Step: 552... Loss: 0.336124... Val Loss: 0.812114 Val Acc: 0.500000\n",
            "Epoch: 35/50... Step: 556... Loss: 0.315457... Val Loss: 0.807803 Val Acc: 0.505000\n",
            "Epoch: 35/50... Step: 560... Loss: 0.324038... Val Loss: 0.772711 Val Acc: 0.540000\n",
            "Epoch: 36/50... Step: 564... Loss: 0.314310... Val Loss: 0.757596 Val Acc: 0.555000\n",
            "Epoch: 36/50... Step: 568... Loss: 0.315186... Val Loss: 0.836859 Val Acc: 0.475000\n",
            "Epoch: 36/50... Step: 572... Loss: 0.325939... Val Loss: 0.810301 Val Acc: 0.500000\n",
            "Epoch: 36/50... Step: 576... Loss: 0.314675... Val Loss: 0.801867 Val Acc: 0.510000\n",
            "Epoch: 37/50... Step: 580... Loss: 0.314636... Val Loss: 0.856966 Val Acc: 0.455000\n",
            "Epoch: 37/50... Step: 584... Loss: 0.325563... Val Loss: 0.802393 Val Acc: 0.510000\n",
            "Epoch: 37/50... Step: 588... Loss: 0.314530... Val Loss: 0.787634 Val Acc: 0.525000\n",
            "Epoch: 37/50... Step: 592... Loss: 0.334501... Val Loss: 0.837333 Val Acc: 0.475000\n",
            "Epoch: 38/50... Step: 596... Loss: 0.313899... Val Loss: 0.817882 Val Acc: 0.495000\n",
            "Epoch: 38/50... Step: 600... Loss: 0.314181... Val Loss: 0.837359 Val Acc: 0.475000\n",
            "Epoch: 38/50... Step: 604... Loss: 0.315043... Val Loss: 0.801767 Val Acc: 0.510000\n",
            "Epoch: 38/50... Step: 608... Loss: 0.324122... Val Loss: 0.807025 Val Acc: 0.505000\n",
            "Epoch: 39/50... Step: 612... Loss: 0.325119... Val Loss: 0.836266 Val Acc: 0.475000\n",
            "Epoch: 39/50... Step: 616... Loss: 0.314858... Val Loss: 0.836999 Val Acc: 0.475000\n",
            "Epoch: 39/50... Step: 620... Loss: 0.315341... Val Loss: 0.767463 Val Acc: 0.545000\n",
            "Epoch: 39/50... Step: 624... Loss: 0.324608... Val Loss: 0.832284 Val Acc: 0.480000\n",
            "Epoch: 40/50... Step: 628... Loss: 0.324232... Val Loss: 0.822436 Val Acc: 0.490000\n",
            "Epoch: 40/50... Step: 632... Loss: 0.315497... Val Loss: 0.826895 Val Acc: 0.485000\n",
            "Epoch: 40/50... Step: 636... Loss: 0.316035... Val Loss: 0.814231 Val Acc: 0.495000\n",
            "Epoch: 40/50... Step: 640... Loss: 0.314180... Val Loss: 0.812407 Val Acc: 0.500000\n",
            "Epoch: 41/50... Step: 644... Loss: 0.314497... Val Loss: 0.797117 Val Acc: 0.515000\n",
            "Epoch: 41/50... Step: 648... Loss: 0.314660... Val Loss: 0.837156 Val Acc: 0.475000\n",
            "Epoch: 41/50... Step: 652... Loss: 0.322531... Val Loss: 0.793571 Val Acc: 0.515000\n",
            "Epoch: 41/50... Step: 656... Loss: 0.313961... Val Loss: 0.807722 Val Acc: 0.505000\n",
            "Epoch: 42/50... Step: 660... Loss: 0.334467... Val Loss: 0.762390 Val Acc: 0.550000\n",
            "Epoch: 42/50... Step: 664... Loss: 0.315075... Val Loss: 0.806897 Val Acc: 0.505000\n",
            "Epoch: 42/50... Step: 668... Loss: 0.325927... Val Loss: 0.821490 Val Acc: 0.490000\n",
            "Epoch: 42/50... Step: 672... Loss: 0.315738... Val Loss: 0.821292 Val Acc: 0.490000\n",
            "Epoch: 43/50... Step: 676... Loss: 0.313924... Val Loss: 0.792619 Val Acc: 0.520000\n",
            "Epoch: 43/50... Step: 680... Loss: 0.323826... Val Loss: 0.812152 Val Acc: 0.500000\n",
            "Epoch: 43/50... Step: 684... Loss: 0.314345... Val Loss: 0.797501 Val Acc: 0.515000\n",
            "Epoch: 43/50... Step: 688... Loss: 0.315268... Val Loss: 0.777843 Val Acc: 0.535000\n",
            "Epoch: 44/50... Step: 692... Loss: 0.314037... Val Loss: 0.772616 Val Acc: 0.540000\n",
            "Epoch: 44/50... Step: 696... Loss: 0.313804... Val Loss: 0.782990 Val Acc: 0.530000\n",
            "Epoch: 44/50... Step: 700... Loss: 0.332010... Val Loss: 0.827933 Val Acc: 0.485000\n",
            "Epoch: 44/50... Step: 704... Loss: 0.314173... Val Loss: 0.807528 Val Acc: 0.505000\n",
            "Epoch: 45/50... Step: 708... Loss: 0.324562... Val Loss: 0.822542 Val Acc: 0.490000\n",
            "Epoch: 45/50... Step: 712... Loss: 0.313886... Val Loss: 0.817707 Val Acc: 0.495000\n",
            "Epoch: 45/50... Step: 716... Loss: 0.316309... Val Loss: 0.869849 Val Acc: 0.440000\n",
            "Epoch: 45/50... Step: 720... Loss: 0.315161... Val Loss: 0.860916 Val Acc: 0.450000\n",
            "Epoch: 46/50... Step: 724... Loss: 0.314785... Val Loss: 0.816315 Val Acc: 0.495000\n",
            "Epoch: 46/50... Step: 728... Loss: 0.314142... Val Loss: 0.802554 Val Acc: 0.510000\n",
            "Epoch: 46/50... Step: 732... Loss: 0.314739... Val Loss: 0.822205 Val Acc: 0.490000\n",
            "Epoch: 46/50... Step: 736... Loss: 0.323811... Val Loss: 0.817811 Val Acc: 0.495000\n",
            "Epoch: 47/50... Step: 740... Loss: 0.315066... Val Loss: 0.793785 Val Acc: 0.520000\n",
            "Epoch: 47/50... Step: 744... Loss: 0.313749... Val Loss: 0.792958 Val Acc: 0.520000\n",
            "Epoch: 47/50... Step: 748... Loss: 0.324496... Val Loss: 0.812278 Val Acc: 0.500000\n",
            "Epoch: 47/50... Step: 752... Loss: 0.314321... Val Loss: 0.843186 Val Acc: 0.470000\n",
            "Epoch: 48/50... Step: 756... Loss: 0.314100... Val Loss: 0.807575 Val Acc: 0.505000\n",
            "Epoch: 48/50... Step: 760... Loss: 0.313775... Val Loss: 0.797902 Val Acc: 0.515000\n",
            "Epoch: 48/50... Step: 764... Loss: 0.314657... Val Loss: 0.812386 Val Acc: 0.500000\n",
            "Epoch: 48/50... Step: 768... Loss: 0.314195... Val Loss: 0.817830 Val Acc: 0.495000\n",
            "Epoch: 49/50... Step: 772... Loss: 0.314261... Val Loss: 0.818520 Val Acc: 0.495000\n",
            "Epoch: 49/50... Step: 776... Loss: 0.314511... Val Loss: 0.797887 Val Acc: 0.515000\n",
            "Epoch: 49/50... Step: 780... Loss: 0.313536... Val Loss: 0.798046 Val Acc: 0.515000\n",
            "Epoch: 49/50... Step: 784... Loss: 0.324726... Val Loss: 0.811915 Val Acc: 0.500000\n",
            "Epoch: 50/50... Step: 788... Loss: 0.313952... Val Loss: 0.847612 Val Acc: 0.465000\n",
            "Epoch: 50/50... Step: 792... Loss: 0.313659... Val Loss: 0.807911 Val Acc: 0.505000\n",
            "Epoch: 50/50... Step: 796... Loss: 0.313897... Val Loss: 0.807822 Val Acc: 0.505000\n",
            "Epoch: 50/50... Step: 800... Loss: 0.314219... Val Loss: 0.837362 Val Acc: 0.475000\n",
            "Test Data Result:\n",
            "Val Loss: 0.830463 Val Acc: 0.602500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDU9mNi4ZKF1",
        "colab_type": "text"
      },
      "source": [
        "# **RNN with BaseLine Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyRf0MyPYShJ",
        "colab_type": "code",
        "outputId": "fda9eb15-30d5-4993-9804-78705a305863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "#Defining the Model\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden\n",
        "    \n",
        "    \n",
        "# Instantiating the model with hyperparams\n",
        "vocab_size = len(vocab)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 128\n",
        "n_layers = 5\n",
        "\n",
        "print(\"\\nModel Structure:\")\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)\n",
        "\n",
        "\n",
        "# Defining the Loss Functions and Optimizer\n",
        "lr=0.001\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# Training the Model with the number of Epochs\n",
        "print(\"\\nModel Training:\")\n",
        "epochs = 4 \n",
        "counter = 0\n",
        "print_every = 4\n",
        "clip=5 \n",
        "\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        h = tuple([each.data for each in h])\n",
        "        net.zero_grad()\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if counter % print_every == 0:\n",
        "            num_correct = 0\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                pred = torch.round(output.squeeze())\n",
        "\n",
        "                correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "                correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "                num_correct += np.sum(correct)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            val_acc = num_correct/len(valid_loader.dataset)\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(np.mean(val_acc)))\n",
        "            \n",
        "\n",
        "print(\"\\nTest Data Result:\")\n",
        "for inputs, labels in test_loader:\n",
        "    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    output, val_h = net(inputs, val_h)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "val_acc = num_correct/len(valid_loader.dataset)\n",
        "net.train()\n",
        "print(\"Test Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "      \"Test Acc: {:.6f}\".format(np.mean(val_acc)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model Structure:\n",
            "SentimentRNN(\n",
            "  (embedding): Embedding(39401, 400)\n",
            "  (lstm): LSTM(400, 128, num_layers=5, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n",
            "\n",
            "Model Training:\n",
            "Epoch: 1/4... Step: 4... Loss: 0.700895... Val Loss: 0.693502 Val Acc: 0.495000\n",
            "Epoch: 1/4... Step: 8... Loss: 0.691015... Val Loss: 0.693109 Val Acc: 0.505000\n",
            "Epoch: 1/4... Step: 12... Loss: 0.695237... Val Loss: 0.693110 Val Acc: 0.505000\n",
            "Epoch: 1/4... Step: 16... Loss: 0.694471... Val Loss: 0.693220 Val Acc: 0.495000\n",
            "Epoch: 2/4... Step: 20... Loss: 0.695540... Val Loss: 0.693118 Val Acc: 0.505000\n",
            "Epoch: 2/4... Step: 24... Loss: 0.691718... Val Loss: 0.693203 Val Acc: 0.495000\n",
            "Epoch: 2/4... Step: 28... Loss: 0.691123... Val Loss: 0.693209 Val Acc: 0.495000\n",
            "Epoch: 2/4... Step: 32... Loss: 0.694246... Val Loss: 0.693291 Val Acc: 0.495000\n",
            "Epoch: 3/4... Step: 36... Loss: 0.694573... Val Loss: 0.693548 Val Acc: 0.495000\n",
            "Epoch: 3/4... Step: 40... Loss: 0.694510... Val Loss: 0.693305 Val Acc: 0.495000\n",
            "Epoch: 3/4... Step: 44... Loss: 0.691348... Val Loss: 0.693134 Val Acc: 0.505000\n",
            "Epoch: 3/4... Step: 48... Loss: 0.692420... Val Loss: 0.693160 Val Acc: 0.495000\n",
            "Epoch: 4/4... Step: 52... Loss: 0.693938... Val Loss: 0.693105 Val Acc: 0.505000\n",
            "Epoch: 4/4... Step: 56... Loss: 0.693487... Val Loss: 0.693099 Val Acc: 0.505000\n",
            "Epoch: 4/4... Step: 60... Loss: 0.695458... Val Loss: 0.693104 Val Acc: 0.505000\n",
            "Epoch: 4/4... Step: 64... Loss: 0.692880... Val Loss: 0.693253 Val Acc: 0.495000\n",
            "Val Loss: 0.693134 Val Acc: 0.622500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qF33kpYYo35",
        "colab_type": "text"
      },
      "source": [
        "# **RNN with BaseLine Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9n_PuOYXBIS",
        "colab_type": "code",
        "outputId": "e7e5fc34-5351-4e4d-ac4d-d904c3e26077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "embed_size = 300 \n",
        "max_features = len(vocab) + 1\n",
        "maxlen = 70 \n",
        "batch_size = 100 \n",
        "n_epochs = 5\n",
        "n_splits = 5 \n",
        "SEED = 10\n",
        "debug = 0\n",
        "embedding_matrix = encoded_review\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "        \n",
        "        self.supports_masking = True\n",
        "\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        \n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.kaiming_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "        \n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        feature_dim = self.feature_dim \n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, feature_dim), \n",
        "            self.weight\n",
        "        ).view(-1, step_dim)\n",
        "        \n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "            \n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "        \n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)\n",
        "\n",
        "class Attention_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Attention_Net, self).__init__()\n",
        "        drp = 0.1\n",
        "        self.embedding = nn.Embedding(max_features, embed_size)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
        "        self.lstm = nn.LSTM(embed_size, 128, bidirectional=True, batch_first=True)\n",
        "        self.lstm2 = nn.GRU(128*2, 64, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.attention_layer = Attention(128, maxlen)\n",
        "        \n",
        "        self.linear = nn.Linear(64*2 , 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))\n",
        "        h_lstm, _ = self.lstm(h_embedding)\n",
        "        h_lstm, _ = self.lstm2(h_lstm)\n",
        "        h_lstm_atten = self.attention_layer(h_lstm)\n",
        "        conc = self.relu(self.linear(h_lstm_atten))\n",
        "        out = self.out(conc)\n",
        "        return out\n",
        "    \n",
        "model = Attention_Net()\n",
        "print(model)\n",
        "\n",
        "# Defining Loss function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "\n",
        "\n",
        "\n",
        "# Training the Model with the number of Epochs\n",
        "print(\"\\nModel Training:\")\n",
        "counter = 0\n",
        "epochs = 5\n",
        "print_every = 4\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for data, labels in train_loader:\n",
        "        counter += 1\n",
        "        optimizer.zero_grad()\n",
        "        data = data.float()\n",
        "        labels = labels.float()\n",
        "        output = model.forward(data)\n",
        "        loss = criterion(output.float(), labels.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if counter % print_every == 0:\n",
        "            \n",
        "            num_correct = 0\n",
        "            \n",
        "            val_losses = []\n",
        "            for inputs, labels in valid_loader:\n",
        "                \n",
        "                inputs, labels = inputs.float(), labels.float()\n",
        "\n",
        "                output = model.forward(data)\n",
        "                \n",
        "                pred = torch.round(output.squeeze()) \n",
        "                \n",
        "                actual =[]\n",
        "                for x,y in pred:\n",
        "                    if x == 1:\n",
        "                        actual.append(0)\n",
        "                    else:\n",
        "                        actual.append(1)\n",
        "                actual = torch.FloatTensor(actual)\n",
        "                # compare predictions to true label\n",
        "                correct_tensor = actual.eq(labels.float())\n",
        "                correct = np.squeeze(correct_tensor.numpy())\n",
        "                num_correct += np.sum(correct)\n",
        "                val_loss = criterion(output.float(), labels.long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "            \n",
        "            val_acc = num_correct/len(valid_loader.dataset)\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(np.mean(val_acc)))\n",
        "            \n",
        "            \n",
        "print(\"\\nTest Data Result:\")\n",
        "for inputs, labels in test_loader:\n",
        "                \n",
        "    inputs, labels = inputs.float(), labels.float()\n",
        "\n",
        "    output = model.forward(data)\n",
        "    \n",
        "    pred = torch.round(output.squeeze()) \n",
        "    \n",
        "    actual =[]\n",
        "    for x,y in pred:\n",
        "        if x == 1:\n",
        "            actual.append(0)\n",
        "        else:\n",
        "            actual.append(1)\n",
        "    actual = torch.FloatTensor(actual)\n",
        "    \n",
        "    correct_tensor = actual.eq(labels.float())\n",
        "    correct = np.squeeze(correct_tensor.numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    val_loss = criterion(output.float(), labels.long())\n",
        "\n",
        "    val_losses.append(val_loss.item())\n",
        "\n",
        "\n",
        "val_acc = num_correct/len(valid_loader.dataset)\n",
        "print(\"Test Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "      \"Test Acc: {:.6f}\".format(np.mean(val_acc)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention_Net(\n",
            "  (embedding): Embedding(39401, 300)\n",
            "  (embedding_dropout): Dropout2d(p=0.1, inplace=False)\n",
            "  (lstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
            "  (lstm2): GRU(256, 64, batch_first=True, bidirectional=True)\n",
            "  (attention_layer): Attention()\n",
            "  (linear): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Model Training:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b75959669d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b75959669d63>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mh_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mh_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF6O-FdOuSd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}